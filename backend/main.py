import asyncio
import logging
import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, ConfigDict
from typing import List, Optional, Dict, Any
import uuid
import aiofiles
import openai

# æœåŠ¡å¯¼å…¥
from services.openai_service import OpenAIService
from services.milvus_service import MilvusService
from services.memory_service import MemoryService
from services.emotion_service import EmotionAnalyzer, EventClassifier
from services.time_service import TimeService

# æ–°å¢ï¼šå¯¼å…¥é‰´æƒæ¨¡å—
from auth.api_auth import require_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€æœåŠ¡å®ä¾‹
openai_service = None
milvus_service = None
memory_service = None
emotion_analyzer = None
event_classifier = None
time_service = None

# é…ç½®ä¸Šä¼ ç›®å½•
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# å…è®¸çš„æ–‡ä»¶ç±»å‹
ALLOWED_EXTENSIONS = {
    'image': {'png', 'jpg', 'jpeg', 'gif', 'webp'},
    'document': {'pdf', 'txt', 'doc', 'docx'},
    'audio': {'mp3', 'wav', 'ogg', 'm4a', 'flac', 'webm'}
}

# Pydantic æ¨¡å‹ - ä¿æŒç°æœ‰çš„ + æ·»åŠ æ–°çš„
class Message(BaseModel):
    role: str
    content: str

class UploadedFile(BaseModel):
    id: str
    filename: str
    type: str
    size: int
    path: str

class ChatRequest(BaseModel):
    message: str
    history: List[Message] = []
    files: List[UploadedFile] = []  # æ–°å¢æ–‡ä»¶å­—æ®µ

class ChatResponse(BaseModel):
    response: str
    memories: List[Dict[str, Any]] = []

class HealthResponse(BaseModel):
    model_config = ConfigDict(protected_namespaces=())
    
    status: str
    timestamp: str
    services: Dict[str, bool]
    model_info: Dict[str, str]
    timezone: str

class FileUploadResponse(BaseModel):
    files: List[UploadedFile]

class TranscriptionResponse(BaseModel):
    text: str
    success: bool

# å·¥å…·å‡½æ•°
def get_file_type(filename: str) -> str:
    ext = filename.lower().split('.')[-1]
    for file_type, extensions in ALLOWED_EXTENSIONS.items():
        if ext in extensions:
            return file_type
    return 'other'

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global openai_service, milvus_service, memory_service, emotion_analyzer, event_classifier, time_service
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–å¢å¼ºæœåŠ¡...")
        
        # 1. åˆå§‹åŒ– OpenAI æœåŠ¡ (æ— éœ€è°ƒç”¨ initialize)
        openai_service = OpenAIService()
        logger.info("âœ“ OpenAI æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ– Milvus æœåŠ¡
        milvus_service = MilvusService()
        await milvus_service.initialize()
        logger.info("âœ“ Milvus æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åˆå§‹åŒ–è®°å¿†æœåŠ¡ï¼ˆä½¿ç”¨æ–°çš„çº¯ Milvus ç‰ˆæœ¬ï¼‰
        memory_service = MemoryService()
        memory_service.set_milvus_service(milvus_service)
        logger.info("âœ“ è®°å¿†æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # 4. åˆå§‹åŒ–æ—¶é—´æœåŠ¡
        time_service = TimeService()
        logger.info("âœ“ æ—¶é—´æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # 5. åˆå§‹åŒ–æƒ…ç»ªåˆ†æå’Œäº‹ä»¶åˆ†ç±»
        emotion_analyzer = EmotionAnalyzer()
        event_classifier = EventClassifier()
        logger.info("âœ“ æƒ…ç»ªåˆ†æå’Œäº‹ä»¶åˆ†ç±»æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # 6. é…ç½® OpenAI (ä¸ºè¯­éŸ³è½¬æ–‡æœ¬)
        openai.api_key = os.getenv('OPENAI_API_KEY')
        logger.info("âœ“ OpenAI è¯­éŸ³æœåŠ¡é…ç½®æˆåŠŸ")
        
        logger.info("ğŸš€ æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    finally:
        logger.info("ğŸ”„ æ­£åœ¨å…³é—­æœåŠ¡...")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Project Yuzuriha Enhanced API",
    description="AIèŠå¤©æœåŠ¡ with Enhanced Memory (Milvus Only) + File Upload + Voice",
    version="2.3.0",
    lifespan=lifespan
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡ (ç”¨äºæ–‡ä»¶ä¸Šä¼ )
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# === æ–°å¢çš„æ–‡ä»¶ä¸Šä¼ è·¯ç”± ===
@app.post("/api/upload", response_model=FileUploadResponse, dependencies=[require_api_key()])
async def upload_files(files: List[UploadFile] = File(...)):
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    uploaded_files = []
    
    for file in files:
        if file.size > 10 * 1024 * 1024:  # 10MB limit
            raise HTTPException(status_code=413, detail=f"File {file.filename} is too large")
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        file_ext = file.filename.split('.')[-1] if '.' in file.filename else ''
        safe_filename = f"{file_id}.{file_ext}" if file_ext else file_id
        file_path = os.path.join(UPLOAD_DIR, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶
        async with aiofiles.open(file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
        
        uploaded_files.append(UploadedFile(
            id=file_id,
            filename=file.filename,
            type=get_file_type(file.filename),
            size=file.size,
            path=file_path
        ))
    
    logger.info(f"ä¸Šä¼ äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶")
    return FileUploadResponse(files=uploaded_files)

# === æ–°å¢çš„è¯­éŸ³è½¬æ–‡æœ¬è·¯ç”± ===
@app.post("/api/transcribe", response_model=TranscriptionResponse, dependencies=[require_api_key()])
async def transcribe_audio(audio: UploadFile = File(...)):
    """è¯­éŸ³è½¬æ–‡æœ¬æ¥å£"""
    if not audio.content_type or not audio.content_type.startswith('audio/'):
        raise HTTPException(status_code=400, detail="File must be an audio file")
    
    if audio.size > 25 * 1024 * 1024:  # 25MB limit for Whisper API
        raise HTTPException(status_code=413, detail="Audio file is too large")
    
    try:
        import tempfile
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            content = await audio.read()
            tmp_file.write(content)
            tmp_file.flush()
            
            # ä½¿ç”¨ OpenAI Whisper API è¿›è¡Œè½¬å½•
            with open(tmp_file.name, 'rb') as audio_file:
                client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh"  # æŒ‡å®šä¸­æ–‡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
                )
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_file.name)
        
        logger.info(f"è¯­éŸ³è½¬æ–‡æœ¬æˆåŠŸ: {transcript.text[:50]}...")
        return TranscriptionResponse(
            text=transcript.text,
            success=True
        )
        
    except Exception as e:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        if 'tmp_file' in locals():
            try:
                os.unlink(tmp_file.name)
            except:
                pass
        
        logger.error(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

# === ä¿®æ”¹çš„èŠå¤©æ¥å£ï¼ˆæ”¯æŒæ–‡ä»¶ï¼‰ ===
@app.post("/api/chat", response_model=ChatResponse, dependencies=[require_api_key()])
async def enhanced_chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """å¢å¼ºèŠå¤©æ¥å£ - çº¯ Milvus ç‰ˆæœ¬ + æ–‡ä»¶æ”¯æŒ"""
    try:
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.message[:50]}...")
        
        # å¤„ç†é™„ä»¶ä¿¡æ¯
        file_context = ""
        if request.files:
            file_info = []
            for file in request.files:
                file_info.append(f"- {file.filename} ({file.type}, {file.size} bytes)")
            file_context = f"\n\nç”¨æˆ·ä¸Šä¼ äº†ä»¥ä¸‹é™„ä»¶ï¼š\n" + "\n".join(file_info)
            logger.info(f"åŒ…å« {len(request.files)} ä¸ªé™„ä»¶")
        
        # 1. åˆ›å»ºæŸ¥è¯¢åµŒå…¥
        query_text = request.message + file_context
        query_embedding = await openai_service.create_embedding(query_text)
        
        # 2. æƒ…ç»ªåˆ†æ
        user_emotion = emotion_analyzer.analyze_emotion(request.message)
        user_category, user_confidence = event_classifier.classify_event(request.message)
        
        # 3. ä» Milvus æ£€ç´¢ç›¸å…³è®°å¿†
        milvus_memories = await memory_service.retrieve_relevant_memories(
            query=query_text,  # æ·»åŠ  query å‚æ•°
            query_embedding=query_embedding,
            limit=5
        )
        logger.info(f"ä» Milvus æ£€ç´¢åˆ° {len(milvus_memories)} ä¸ªè®°å¿†")
        
        # 4. è½¬æ¢å†å²æ¶ˆæ¯æ ¼å¼
        conversation_history = [
            {'role': msg.role, 'content': msg.content} 
            for msg in request.history
        ]
        
        # 5. ç”ŸæˆAIå›å¤ï¼ˆåŒ…å«æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼‰
        enhanced_message = query_text  # åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„æ¶ˆæ¯
        response = await openai_service.generate_response(
            enhanced_message,
            memories=milvus_memories,
            conversation_history=conversation_history
        )
        
        # 6. åå°å­˜å‚¨è®°å¿†
        background_tasks.add_task(
            store_conversation_memories,
            enhanced_message,  # å­˜å‚¨åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„æ¶ˆæ¯
            response,
            query_embedding,
            user_emotion,
            user_category,
            user_confidence
        )
        
        logger.info(f"èŠå¤©å¤„ç†å®Œæˆï¼Œç”Ÿæˆå›å¤é•¿åº¦: {len(response)}")
        
        return ChatResponse(
            response=response,
            memories=milvus_memories
        )
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤„ç†é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤„ç†å¤±è´¥: {str(e)}")

# åœ¨ç°æœ‰çš„ @app.post("/api/chat") ä¹‹åæ·»åŠ 
@app.post("/chat", response_model=ChatResponse)
async def chat_legacy(request: ChatRequest, background_tasks: BackgroundTasks):
    """å…¼å®¹æ—§ç‰ˆæœ¬çš„èŠå¤©æ¥å£"""
    return await enhanced_chat(request, background_tasks)
# === ä¿æŒç°æœ‰çš„å…¶ä»–è·¯ç”± ===
async def store_conversation_memories(
    user_message: str,
    ai_response: str,
    query_embedding: List[float],
    user_emotion: Dict[str, float],
    user_category: str,
    user_confidence: float
):
    """å­˜å‚¨å¯¹è¯è®°å¿†åˆ° Milvus"""
    try:
        # å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
        await milvus_service.store_memory(
            text=user_message,
            embedding=query_embedding,
            user_id="marvinli001",
            emotion_weight=user_emotion.get('emotion_weight', 0.0),
            event_category=user_category,
            interaction_type="user_message"
        )
        
        # å­˜å‚¨ AI å›å¤
        response_embedding = await openai_service.create_embedding(ai_response)
        await milvus_service.store_memory(
            text=ai_response,
            embedding=response_embedding,
            user_id="marvinli001",
            emotion_weight=0.7,  # AIå›å¤çš„é»˜è®¤æƒ…ç»ªæƒé‡
            event_category="response",
            interaction_type="ai_response"
        )
        
        logger.info("å¯¹è¯è®°å¿†å­˜å‚¨å®Œæˆ")
        
    except Exception as e:
        logger.error(f"å­˜å‚¨å¯¹è¯è®°å¿†å¤±è´¥: {e}")

@app.get("/health", response_model=HealthResponse)
async def enhanced_health_check():
    """å¢å¼ºçš„å¥åº·æ£€æŸ¥ - ç§»é™¤ SuperMemory"""
    try:
        time_info = time_service.get_time_context()
        model_info = openai_service.get_model_info()
        
        services_status = {
            "openai": openai_service is not None,
            "milvus": milvus_service is not None and milvus_service.client is not None,
            "memory_service": memory_service is not None,
            "time_service": time_service is not None,
            "emotion_analyzer": emotion_analyzer is not None,
            "event_classifier": event_classifier is not None
        }
        
        overall_status = "healthy" if all([
            services_status["openai"],
            services_status["milvus"],
            services_status["memory_service"],
            services_status["time_service"],
            services_status["emotion_analyzer"],
            services_status["event_classifier"]
        ]) else "unhealthy"
        
        return HealthResponse(
            status=overall_status,
            timestamp=time_info['current_time'],
            services=services_status,
            model_info=model_info,
            timezone=time_info['timezone']
        )
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

@app.get("/api/stats")
async def get_memory_stats():
    """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = await memory_service.get_memory_stats()
        return {
            "status": "success",
            "data": stats,
            "backend": "milvus_only"
        }
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    time_info = time_service.get_time_context()
    
    return {
        "message": "Project Yuzuriha Enhanced API",
        "version": "2.3.0",
        "status": "running",
        "current_time": time_info['current_time'],
        "memory_backend": "milvus_only",
        "supermemory_removed": True,
        "features": [
            "å¢å¼ºè®°å¿†ç³»ç»Ÿ (çº¯Milvus)",
            "æƒ…ç»ªåˆ†æ",
            "äº‹ä»¶åˆ†ç±»", 
            "æ—¶é—´æ„ŸçŸ¥",
            "è¯­ä¹‰æœç´¢",
            "Zilliz Cloud Milvus",
            "æ–‡ä»¶ä¸Šä¼ ",
            "è¯­éŸ³è½¬æ–‡æœ¬"
        ]
    }

@app.post("/upload", response_model=FileUploadResponse)
async def upload_files(files: List[UploadFile] = File(...)):
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    try:
        uploaded_files = []
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            file_type = get_file_type(file.filename)
            if file_type == 'other':
                raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}")
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            file_id = str(uuid.uuid4())
            file_extension = file.filename.split('.')[-1]
            unique_filename = f"{file_id}.{file_extension}"
            file_path = os.path.join(UPLOAD_DIR, unique_filename)
            
            # ä¿å­˜æ–‡ä»¶
            async with aiofiles.open(file_path, 'wb') as out_file:
                content = await file.read()
                await out_file.write(content)
            
            # åˆ›å»ºæ–‡ä»¶è®°å½•
            uploaded_file = UploadedFile(
                id=file_id,
                filename=file.filename,
                type=file_type,
                size=len(content),
                path=file_path
            )
            uploaded_files.append(uploaded_file)
            
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename} -> {unique_filename}")
        
        return FileUploadResponse(files=uploaded_files)
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(file: UploadFile = File(...)):
    """éŸ³é¢‘è½¬æ–‡æœ¬æ¥å£"""
    try:
        # éªŒè¯æ˜¯å¦ä¸ºéŸ³é¢‘æ–‡ä»¶
        file_type = get_file_type(file.filename)
        if file_type != 'audio':
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒéŸ³é¢‘æ–‡ä»¶è½¬å½•")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file.filename.split('.')[-1]}") as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file.flush()
            
            # è°ƒç”¨ OpenAI Whisper API
            with open(tmp_file.name, 'rb') as audio_file:
                transcript = await openai_service.transcribe_audio(audio_file)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file.name)
            
            logger.info(f"éŸ³é¢‘è½¬å½•æˆåŠŸ: {file.filename}")
            
            return TranscriptionResponse(
                text=transcript,
                success=True
            )
        
    except Exception as e:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        if 'tmp_file' in locals():
            try:
                os.unlink(tmp_file.name)
            except:
                pass
        
        logger.error(f"è¯­éŸ³è½¬æ–‡æœ¬å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)